{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3wVOJC3bQlDlQJ/qD9N3c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Pytorch (Autograd Practice)"],"metadata":{"id":"J5qUwehRdu-t"}},{"cell_type":"markdown","source":["## The Computation Graph (The \"Brain\" of Autograd)\n","- The core concept behind Autograd is the dynamic computation graph (DCG).\n","- When you perform an operation on a PyTorch tensor, Autograd implicitly builds a graph where:\n","  - Nodes represent the Tensors.\n","  - Edges represent the functions or operations applied to those Tensors (e.g., addition, multiplication, or $x^2$).\n","  - This graph is built dynamically, meaning it is rebuilt for **every single forward pass**."],"metadata":{"id":"AIARU6rQoT8T"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"pWW60v99dru4","executionInfo":{"status":"ok","timestamp":1765121980363,"user_tz":-300,"elapsed":6877,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["## Tensors and requires_grad\n","- The **requires_grad** flag dictates whether Autograd should track operations on a tensor.\n","- Leaf Nodes (Inputs/Parameters): For tensors that are user inputs (X) or model parameters (W, b), set requires_grad=True.\n","- Intermediate/Output Nodes (y): These tensors automatically **inherit** the requires_grad=True flag if any tensor used to create them had requires_grad=True.\n","- We can change this flag in-place using .requires_grad_():\n","\n","``` Python\n","W = torch.randn(5, 5) # requires_grad=False by default\n","W.requires_grad_(True) # Now tracking operations on W\n","# Note using '_ Underscore' means we are changing the option\n","# IN-PLACE (within the same tensor and not creating another tensor)\n","\n","# OR\n","W = torch.randn(5, 5, requires_grad=True)\n","```\n","\n","## Gradient Accumulation\n","- Gradients **accumulate** in the .grad attribute. This is the most common pitfall in PyTorch.\n","- Every time we call .backward(), the new gradients are **added** to any existing values in the .grad attribute.\n","  - Why? In advanced scenarios like Recurrent Neural Networks (RNNs) or accumulating gradients over large batches, this accumulation is desirable.\n","  - The Fix: We must **manually zero out the gradients** before running a new backward pass, usually using **optimizer.zero_grad()**.\n","\n","``` Python\n","# Before starting a new batch or epoch\n","# optimizer.zero_grad()\n","X.grad.zero_() # Manually zeroing the gradient for the next calculation IN-PLACE\n","```"],"metadata":{"id":"Ddbh1GCGq26w"}},{"cell_type":"markdown","source":["## Gradient Calculation and Scope\n","- In our example Below: y.backward(), we are finding the gradient of the final scalar value (y) with respect to its dependency (X).\n","- In PyTorch, calling tensor.backward() effectively computes the gradient of that tensor with respect to the leaf nodes (X).\n","- Our Code: Since $y = X^2$, calling y.backward() computes $\\frac{d y}{d X} = 2X$. With $X=3.0$, $X.\\text{grad}$ becomes $2 \\times 3.0 = 6.0$.\n","- Neural Networks: In machine learning, $y$ is usually a loss function ($L$). When we call $L.\\text{backward()}$, we are computing $\\frac{\\partial L}{\\partial \\text{Weights}}$, which is used for the optimization step. The gradient of the loss is what truly matters.\n","### The grad_fn Attribute\n","- The grad_fn attribute is crucial and its existence indicates that Autograd is tracking the computation for this tensor.\n","- If a tensor is created as a result of an operation, it will have a grad_fn property, which references the function that created it (e.g., <PowBackward0>).\n","- If a tensor is a leaf node (like our initial X with requires_grad=True), it has no grad_fn. It is the starting point of the graph."],"metadata":{"id":"kch3wPVSpQr2"}},{"cell_type":"code","source":["X = torch.tensor(3.0, requires_grad=True)\n","print(X)\n","y = X**2\n","print(y)\n","y.backward()\n","print(X.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SpH1jFOqd7KH","executionInfo":{"status":"ok","timestamp":1765126833012,"user_tz":-300,"elapsed":7,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}},"outputId":"3e978ac6-911b-4090-9e38-5892c9009abd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(3., requires_grad=True)\n","tensor(9., grad_fn=<PowBackward0>)\n","tensor(6.)\n"]}]},{"cell_type":"markdown","source":["## Disabling Autograd Tracking\n","- When performing operations that should not be tracked by Autograd (e.g., during model evaluation, **testing**, or weight updates), use the torch.no_grad() context manager.\n","  - This reduces memory consumption by preventing the DCG from being built.\n","  - It significantly speeds up forward passes where gradients aren't needed.\n","### Using `requires_grad_(False)` functionality\n","``` Python\n","z = w*x + b\n","model.eval()\n","# During Testing\n","w.requires_grad_(False)\n","b.requires_grad_(False)\n","z = w*x + b\n","# The 'z' tensor will have requires_grad=False AND\n","# .backward() functionality will not run\n","```\n","### Using `.detach()` functionality\n","- This Method creates a new tensor with the same values from the tensor on which this method is used BUT with requires_grad set as FALSE.\n","``` Python\n","z = w*x + b\n","model.eval()\n","# During Testing\n","w1 = w.detatch()\n","b1 = b.detatch()\n","z1 = w1*x + b\n","# The 'z1' tensor will have requires_grad=False AND\n","# .backward() functionality will not run\n","```\n","### The `with torch.no_grad()`: Context Manager\n","``` Python\n","# During model evaluation or validation\n","model.eval()\n","with torch.no_grad():\n","    predictions = model(input_data)\n","    # The 'predictions' tensor will have requires_grad=False\n","```\n"],"metadata":{"id":"hHoWpGRKw52c"}},{"cell_type":"markdown","source":["### Major Example"],"metadata":{"id":"M6YZT3AU0go7"}},{"cell_type":"code","source":["# Major Example of A Simple Perceptron of Logistic Regression\n","\n","# Inputs\n","x = torch.tensor(6.7) # Input Feature - Lets say IQ Score of Student - 6.7\n","y = torch.tensor(0.0) # True Label - Lets Say Student Didnot get the job\n","\n","# INITIALIZATION OF WEIGHT AND BIAS (ORIGINALLY WILL BE RANDOMLY INITIALIZED)\n","w = torch.tensor(1.0)\n","b = torch.tensor(0.0)\n","\n","# ALL ABOVE ARE ACTUALLY SCALARS"],"metadata":{"id":"W7fpcRT3zJRo","executionInfo":{"status":"ok","timestamp":1765127897285,"user_tz":-300,"elapsed":4,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Binary Entropy Loss Calculator for Scalars (Manually)\n","def binary_cross_entropy_loss(pred,target):\n","  epsilon = 1e-8\n","  # epsilon is A Self Made Small value used to clamp the predictions\n","  # within the range [1 - epsilon, epsilon] ([max, min]). We do this\n","  # to prevent any prediction value to be zero by any chance so that\n","  # the log(0)=undefined scenario could be avoided in cross_entropy loss\n","  # because it uses the logs to calculate the loss\n","  prediction = torch.clamp(pred,epsilon,1-epsilon)\n","  return -(target * torch.log(pred) + (1-target) * (torch.log(1-pred)))"],"metadata":{"id":"DO1fue0B0j7C","executionInfo":{"status":"ok","timestamp":1765128410208,"user_tz":-300,"elapsed":347,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Doing Sample Forward Passing (Manually)\n","z = w*x + b               # Weighted Sum Calculation\n","y_pred = torch.sigmoid(z) # Probability Mapping\n","# Computing Cross_Entropy Loss (Manually)\n","loss = binary_cross_entropy_loss(y_pred,y)\n","print(f\"Z = {z}, y_pred = {y_pred}, loss = {loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD3mD2-h1-Q4","executionInfo":{"status":"ok","timestamp":1765128960921,"user_tz":-300,"elapsed":34,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}},"outputId":"5abe7c48-bffa-46ce-89b2-1596609bfb42"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Z = 6.699999809265137, y_pred = 0.998770534992218, loss = 6.701176166534424\n"]}]},{"cell_type":"code","source":["# Partial Differential Or Gradients Calculations (Manually) - (Digma Symbol can't be written so using 'd' so don't confuse in differential and partial differential)\n","# 1. dL/d(y_pred): Change in Loss with respect to the prediction (y_pred)\n","dloss_dy_pred = (y_pred - y)/(y_pred * (1-y_pred))\n","\n","# 2. d(y_pred)/dz: Change in prediction (y_pred) with respect to z (sigmoid mapping)\n","dy_pred_dz = y_pred * (1-y_pred)\n","\n","# 3. dz/dw and dz/db: Change in z with respect to weight and bias (our leaf nodes in computation graph)\n","dz_dw = x\n","dz_db = 1   # 1 means bias contribute directly to z and don't depent any other variable\n","\n","dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n","dL_db = dloss_dy_pred * dy_pred_dz * dz_db\n","\n","print(f\"Manual Gradient of Loss w.r.t weight (w): {dL_dw}\")\n","print(f\"Manual Gradient of Loss w.r.t bias (b): {dL_db}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-WEN-IO2GKT","executionInfo":{"status":"ok","timestamp":1765129111105,"user_tz":-300,"elapsed":39,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}},"outputId":"67c6b705-6b36-45ce-ff53-30296351649d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual Gradient of Loss w.r.t weight (w): 6.691762447357178\n","Manual Gradient of Loss w.r.t bias (b): 0.998770534992218\n"]}]},{"cell_type":"code","source":["# NOW USING AUTOGRAD WE CAN DO THE SAME WITHIN FEW LINES OF CODE\n","x = torch.tensor(6.7)\n","y = torch.tensor(0.0)\n","# NOTE WE ARE NOT ENABLING AUTOGRAD OPTION FOR x (feature) and y (label)\n","# BECAUSE WE DON'T NEED OR REQUIRE TO TRACK THE OPERATIONS WITH RESPECT TO\n","# FEATURE AND LABEL (AND NORMALY WE WON'T EVEN IN FUTURE) INSTEAD OUR\n","# POINT OF CONCERN IS THE SELF INITIALIZED WEIGHT AND BIAS - ONE OF THE\n","# REASON FOR THIS IS THAT FEATURE AND LABEL ARE DATA PROVIDED AND WE\n","# CAN'T OR DON'T WANT TO MANIPULATE THE DATA INSTEAD WE WANT TO MANIPULATE\n","# OUR OWN ASSUMPTIONS TO WORK BEST ON THE DATA.\n","w = torch.tensor(1.0, requires_grad=True)\n","b = torch.tensor(0.0, requires_grad=True)"],"metadata":{"id":"dDxq7Sy-46P7","executionInfo":{"status":"ok","timestamp":1765129456203,"user_tz":-300,"elapsed":17,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Now doing other things\n","z = w*x + b               # Weighted Sum Calculation\n","y_pred = torch.sigmoid(z) # Probability Mapping\n","# Computing Cross_Entropy Loss (Manually)\n","loss = binary_cross_entropy_loss(y_pred,y)\n","print(f\"Z = {z}, y_pred = {y_pred}, loss = {loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sbe04L2n6gg_","executionInfo":{"status":"ok","timestamp":1765130269633,"user_tz":-300,"elapsed":25,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}},"outputId":"93f5c2e2-d5f4-474b-bc64-9a816137e350"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Z = 6.699999809265137, y_pred = 0.998770534992218, loss = 6.701176166534424\n"]}]},{"cell_type":"code","source":["# Now doing backward propagation and checking the results\n","loss.backward()\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"udm8U5SU7FBz","executionInfo":{"status":"ok","timestamp":1765130270966,"user_tz":-300,"elapsed":27,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}},"outputId":"0ac1b624-d8d0-4110-b062-64c27560012a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(6.6918)\n","tensor(0.9988)\n"]}]},{"cell_type":"code","source":["# Now Clearing the Gradients\n","w.grad.zero_()\n","b.grad.zero_()\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXtFvmVp7PAj","executionInfo":{"status":"ok","timestamp":1765130328073,"user_tz":-300,"elapsed":36,"user":{"displayName":"Muhammad Hamza","userId":"17024478579669502554"}},"outputId":"d5c5ce70-a4e8-4dd5-da83-c0cff3736ae9"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.)\n","tensor(0.)\n"]}]}]}